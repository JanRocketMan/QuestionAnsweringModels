{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words extracted. Total number: 33012\n",
      "Number of pre-trained: 32416\n"
     ]
    }
   ],
   "source": [
    "from cbt_preprocessing import CBTProcessor\n",
    "\n",
    "# give path to text corpus and embeddings\n",
    "data_loader = CBTProcessor('data/cbt_train.txt', 'data/glove.6B.50d.txt', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare for batch generation on train and val splits\n",
    "data_loader.fit_on_texts('data/cbtest_CN_train.txt', 'train')\n",
    "data_loader.fit_on_texts('data/cbtest_CN_valid_2000ex.txt', 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOC:\n",
      "it would be a singular thing for me to keep it . it 's not to be supposed that i would be any hindrance to gentlemen in your situation ; that would be a singular thing ! '' cries he , and began to pull gold out of his pocket with a mighty red face . alan said nothing , only looked on the ground . `` will you step to the door with me , sir ? '' said i. cluny said he would be very glad , and followed me readily enough , but he looked flustered and put out . `` and now , sir , '' says i , `` i must first acknowledge your generosity . '' `` nonsensical nonsense ! '' cries cluny . `` where 's the generosity ? this is just a most unfortunate affair ; but what would ye have me do -- boxed up in this of a cage of mine -- but just set my friends to the cartes , when i can get them ? and if they lose , of course , it 's not to be supposed -- '' and here he came to a pause . `` yes , '' said i , `` if they lose , you give them back their money ; and if they win , they carry away yours in their pouches ! i have said before that i grant your generosity ; but to me , sir , it 's a very painful thing to be placed in this position . '' there was a little silence , in which cluny seemed always as if he was about to speak , but said nothing . all the time he grew redder and redder in the face . `` i am a young man , '' said i , `` and i ask your advice . advise me as you would your son . my friend fairly lost his money , after having fairly gained a far greater sum of yours ; can i accept it back again ? would that be the right part for me to play ?\n",
      "----------\n",
      "QUERY:\n",
      "whatever i do , you can see for yourself it must be hard upon a \u001b[31mXXXXX\u001b[0m of any pride . ''\n",
      "----------\n",
      "ANSWER:\n",
      "man\n"
     ]
    }
   ],
   "source": [
    "# check batch sampling\n",
    "ex_batch_train = data_loader.sample_batch('train', 32)\n",
    "data_loader.show_example(ex_batch_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc shape: (32, 1000)\n",
      "Query shape: (32, 150)\n",
      "----------\n",
      "Cands shape: (32, 10)\n",
      "----------\n",
      "Answer shape: (32,)\n",
      "----------\n",
      "Mask shape: (32, 1000, 150)\n"
     ]
    }
   ],
   "source": [
    "# each batch contains:\n",
    "# docs and queries\n",
    "ex_D_train, ex_Q_train = ex_batch_train[0], ex_batch_train[1]\n",
    "print('Doc shape:', ex_D_train.shape)\n",
    "print('Query shape:', ex_Q_train.shape)\n",
    "# 10 possible candidates\n",
    "ex_C_train = ex_batch_train[2]\n",
    "print('-'*10)\n",
    "print('Cands shape:', ex_C_train.shape)\n",
    "# real answers\n",
    "ex_A_train = ex_batch_train[3]\n",
    "print('-'*10)\n",
    "print('Answer shape:', ex_A_train.shape)\n",
    "# Doc2Query Masks\n",
    "ex_mask_train = ex_batch_train[4]\n",
    "print('-'*10)\n",
    "print('Mask shape:', ex_mask_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# 3. Model\n",
    "We'll implement Attention-over-attention reader, as described in https://arxiv.org/abs/1607.04423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Attention_over_Attention_Reader():\n",
    "    def __init__(self, dict_size, embedding_dim=100, hidden_state_dim=50, \n",
    "                 D_max=1000, Q_max=100, l2_w=0.0001, dropout_rate=0.1,\n",
    "                 adam_w=0.001, grad_clip=5):\n",
    "        self.dict_size = dict_size\n",
    "        self.emb_dim, self.rnn_dim = embedding_dim, hidden_state_dim\n",
    "        self.D_max, self.Q_max = D_max, Q_max\n",
    "        self.l2_w, self.adam_w = l2_w, adam_w\n",
    "        self.dropout, self.grad_clip = 1 - dropout_rate, grad_clip\n",
    "        self.comp_graph = tf.Graph()\n",
    "        self.init_graph()\n",
    "        \n",
    "        # initialize embedding matrix and rnn\n",
    "        self.sess = tf.Session(graph=self.comp_graph)\n",
    "        self.sess.run(self.init_all_op)\n",
    "    \n",
    "    def init_params(self):\n",
    "        self.D = tf.placeholder(tf.int32, [None, self.D_max], name='Document')\n",
    "        self.lenD = tf.placeholder(tf.int32, [None], name='Document_length')\n",
    "        self.Q = tf.placeholder(tf.int32, [None, self.Q_max], name='Query')\n",
    "        self.lenQ = tf.placeholder(tf.int32, [None], name='Query_length')\n",
    "        self.DQ_mask = tf.placeholder(tf.float32, \n",
    "                                      [None, self.D_max, self.Q_max], name='Document_Query_mask')\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.D_max], name='Answer_mask')\n",
    "        \n",
    "        self.embedding_mtx = tf.Variable(\n",
    "            tf.random_uniform([self.dict_size, self.emb_dim], \n",
    "                              -0.05, 0.05, dtype=tf.float32), name='Embedding_matrix')\n",
    "    \n",
    "    def process_text(self):\n",
    "        # Embed input texts\n",
    "        embedded_D = tf.nn.dropout(\n",
    "            tf.nn.embedding_lookup(self.embedding_mtx, self.D, name='Embedded_document'), \n",
    "            self.dropout)\n",
    "        embedded_Q = tf.nn.dropout(\n",
    "            tf.nn.embedding_lookup(self.embedding_mtx, self.Q, name='Embedded_query'), \n",
    "            self.dropout)\n",
    "        # Process Document using bi-GRU\n",
    "        with tf.variable_scope('Document_processor', initializer=tf.orthogonal_initializer()):\n",
    "            fwd_cell = tf.nn.rnn_cell.GRUCell(self.rnn_dim)\n",
    "            bwd_cell = tf.nn.rnn_cell.GRUCell(self.rnn_dim)\n",
    "            \n",
    "            h_out, _ = tf.nn.bidirectional_dynamic_rnn(fwd_cell, bwd_cell, embedded_D, \n",
    "                                         sequence_length=self.lenD, dtype=tf.float32)\n",
    "            self.h_Doc = tf.concat(h_out, 2)\n",
    "        # Process Query using bi-GRU\n",
    "        with tf.variable_scope('Query_processor', initializer=tf.orthogonal_initializer()):\n",
    "            fwd_cell = tf.nn.rnn_cell.GRUCell(self.rnn_dim)\n",
    "            bwd_cell = tf.nn.rnn_cell.GRUCell(self.rnn_dim)\n",
    "            \n",
    "            h_out, _ = tf.nn.bidirectional_dynamic_rnn(fwd_cell, bwd_cell, embedded_Q, \n",
    "                                         sequence_length=self.lenQ, dtype=tf.float32)\n",
    "            self.h_Query = tf.concat(h_out, 2)\n",
    "    \n",
    "    def compute_scores(self):\n",
    "        # get matching scores for Document and Query\n",
    "        M = tf.matmul(self.h_Doc, self.h_Query, transpose_b=True, name='Matching_scores')\n",
    "        # apply softmax Document-wise\n",
    "        self.alpha = softmax(M, 1, self.DQ_mask, name='Query_to_Document_attention')\n",
    "        # apply softmax Query-wise, then average to get importance of each word in Query\n",
    "        self.beta = tf.reduce_sum(softmax(M, 2, self.DQ_mask), \n",
    "                                  1, keep_dims=True, name='Document_to_Query_attention')\n",
    "        self.beta_imp = tf.div(self.beta, tf.maximum(tf.reduce_sum(self.DQ_mask,axis=1,keep_dims=True),1))\n",
    "        self.s = tf.matmul(self.alpha, self.beta_imp, transpose_b=True, name='Final_scores')\n",
    "    \n",
    "    def init_graph(self):\n",
    "        tf.reset_default_graph()\n",
    "        with self.comp_graph.as_default():\n",
    "            self.init_params()\n",
    "            self.process_text()\n",
    "            self.compute_scores()\n",
    "            # get probability that y is the answer word\n",
    "            with tf.variable_scope('Aggregating_results'):\n",
    "                self.p_y = tf.reduce_sum(\n",
    "                    tf.reduce_sum(self.s, axis=2) * self.y, \n",
    "                    axis=1)\n",
    "            # train to maximize negative log loss of the answer word\n",
    "            self.loss = -tf.reduce_mean(tf.log(tf.maximum(self.p_y, 1e-12)))\n",
    "            self.loss += self.l2_w * tf.nn.l2_loss(self.embedding_mtx)\n",
    "            \n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.adam_w)\n",
    "            # use gradient clipping to avoid exploding\n",
    "            gvs = optimizer.compute_gradients(self.loss)\n",
    "            capped_gvs = [(tf.clip_by_value(grad, -self.grad_clip, self.grad_clip), \n",
    "                           var) for grad, var in gvs]\n",
    "            self.train_op = optimizer.apply_gradients(capped_gvs)\n",
    "            \n",
    "            self.init_all_op = tf.global_variables_initializer()\n",
    "    \n",
    "    def fit(self, data, n_steps, batches_per_step, batch_size=32, valid_data=None):\n",
    "        self.train_losses = []\n",
    "        self.valid_losses = []\n",
    "        for step in tqdm(range(n_steps)):\n",
    "            temp,temp2 = 0,0\n",
    "            for i in range(batches_per_step):\n",
    "                sample_data = sample_batch(data, batch_size, word_to_id,\n",
    "                                                 D_max_len=self.D_max, Q_max_len=self.Q_max)\n",
    "                inputs = [self.D, self.lenD, self.Q, self.lenQ, self.DQ_mask, self.y] \n",
    "                _, iloss = self.sess.run([self.train_op, self.loss], \n",
    "                                         feed_dict={i:d for i,d in zip(inputs,sample_data)})\n",
    "                assert not np.isinf(iloss)\n",
    "                temp += iloss\n",
    "            if valid_data is not None:\n",
    "                sample_data = sample_batch(valid_data, 1000, word_to_id,\n",
    "                                                 D_max_len=self.D_max, Q_max_len=self.Q_max)\n",
    "                inputs = [self.D, self.lenD, self.Q, self.lenQ, self.DQ_mask, self.y]\n",
    "                temp2 = self.sess.run(self.loss, \n",
    "                                      feed_dict={i:d for i,d in zip(inputs,sample_data)})\n",
    "                \n",
    "            self.train_losses.append([temp/batches_per_step])\n",
    "            if step != 0 and self.valid_losses[-1] < temp2 * 0.98:\n",
    "                print('valid loss has reached minimum')\n",
    "                break\n",
    "            self.valid_losses.append([temp2])\n",
    "    \n",
    "    def predict(self, data, batch_size=100):\n",
    "        y_hat = [[0,0]]*len(data)\n",
    "        for step in tqdm(range(0, len(data), batch_size)):\n",
    "            sample_data = sample_batch(data, batch_size, word_to_id,\n",
    "                                      self.D_max, self.Q_max, offset=step)\n",
    "            inputs = [self.D, self.lenD, self.Q, self.lenQ, self.DQ_mask, self.y]\n",
    "            \n",
    "            scores = np.sum(self.sess.run(self.s,\n",
    "                                   feed_dict={i:d for i,d in zip(inputs,sample_data)}),2)\n",
    "            \n",
    "            for i,doc in enumerate(sample_data[0]):\n",
    "                ans = doc[0]\n",
    "                p_ans = 0\n",
    "                p_tot = 0\n",
    "                for word in np.unique(doc):\n",
    "                    p_word = np.sum(scores[i][np.where(doc == word)])\n",
    "                    p_tot += p_word\n",
    "                    if p_word > p_ans:\n",
    "                        ans = word\n",
    "                        p_ans = p_word\n",
    "                y_hat[step + i] = [ans, p_ans]\n",
    "        return np.array(y_hat)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "Att_Reader = Attention_over_Attention_Reader(dictionary_size)\n",
    "tf.summary.FileWriter(\"logs\", Att_Reader.comp_graph).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial accuracy:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:56<00:00,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('initial accuracy:')\n",
    "compute_accuracy(Att_Reader, cat_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "valid_accuracies = []\n",
    "\n",
    "# 32 * 200 * 50 = 320,000\n",
    "# 32 * 100 takes ~5min, \n",
    "total_n_steps = 200\n",
    "batches_per_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [23:55<00:00, 287.06s/it]\n",
      "100%|██████████| 5/5 [20:28<00:00, 245.65s/it]\n",
      " 40%|████      | 2/5 [07:29<11:14, 224.82s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss has reached minimum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Exception in thread Thread-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/usr/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      " 40%|████      | 2/5 [08:06<12:09, 243.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss has reached minimum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_n_steps = 20\n",
    "for steps in range(0, total_n_steps, 5):\n",
    "    Att_Reader.fit(cat_train, 5, batches_per_step, valid_data=cat_valid)\n",
    "    train_losses += Att_Reader.train_losses\n",
    "    valid_losses += Att_Reader.valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [10:02<15:04, 301.37s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss has reached minimum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Exception in thread Thread-16:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/usr/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      " 40%|████      | 2/5 [08:36<12:55, 258.48s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss has reached minimum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [22:45<00:00, 273.09s/it]\n",
      "100%|██████████| 5/5 [53:12<00:00, 638.53s/it]\n"
     ]
    }
   ],
   "source": [
    "total_n_steps = 20\n",
    "for steps in range(0, total_n_steps, 5):\n",
    "    Att_Reader.fit(cat_train, 5, batches_per_step, valid_data=cat_valid)\n",
    "    train_losses += Att_Reader.train_losses\n",
    "    valid_losses += Att_Reader.valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [07:15<29:03, 435.82s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss has reached minimum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Exception in thread Thread-19:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/usr/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|██████████| 5/5 [33:29<00:00, 401.92s/it]\n",
      "100%|██████████| 5/5 [37:48<00:00, 453.76s/it]\n",
      "100%|██████████| 5/5 [37:25<00:00, 449.06s/it]\n"
     ]
    }
   ],
   "source": [
    "total_n_steps = 20\n",
    "for steps in range(0, total_n_steps, 5):\n",
    "    Att_Reader.fit(cat_train, 5, batches_per_step, valid_data=cat_valid)\n",
    "    train_losses += Att_Reader.train_losses\n",
    "    valid_losses += Att_Reader.valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [37:09<00:00, 445.82s/it]\n",
      "100%|██████████| 5/5 [45:20<00:00, 544.05s/it]\n",
      "100%|██████████| 5/5 [52:50<00:00, 634.16s/it]\n",
      "100%|██████████| 5/5 [39:18<00:00, 471.66s/it]\n"
     ]
    }
   ],
   "source": [
    "total_n_steps = 20\n",
    "for steps in range(0, total_n_steps, 5):\n",
    "    Att_Reader.fit(cat_train, 5, batches_per_step, valid_data=cat_valid)\n",
    "    train_losses += Att_Reader.train_losses\n",
    "    valid_losses += Att_Reader.valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [33:09<08:17, 497.44s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss has reached minimum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "Exception in thread Thread-26:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/usr/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|██████████| 5/5 [46:59<00:00, 563.90s/it]\n",
      "100%|██████████| 5/5 [1:03:06<00:00, 757.38s/it]\n",
      " 60%|██████    | 3/5 [35:34<23:43, 711.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss has reached minimum\n"
     ]
    }
   ],
   "source": [
    "total_n_steps = 20\n",
    "for steps in range(0, total_n_steps, 5):\n",
    "    Att_Reader.fit(cat_train, 5, batches_per_step, valid_data=cat_valid)\n",
    "    train_losses += Att_Reader.train_losses\n",
    "    valid_losses += Att_Reader.valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after 100 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Exception in thread Thread-28:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/usr/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|██████████| 20/20 [01:07<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.295\n"
     ]
    }
   ],
   "source": [
    "print('accuracy after 100 steps')\n",
    "compute_accuracy(Att_Reader, cat_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print('accuracy after 100 steps')\n",
    "compute_accuracy(Att_Reader, cat_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.title('Negative log loss', size=25)\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(valid_losses, label='valid')\n",
    "plt.legend(loc='best',fontsize=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
