{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import itertools\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preprocessing\n",
    "(Collect word dictionary, transform each word to dict index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_train = pd.read_csv('data/CBT_CN_train.csv',delimiter=';')\n",
    "sample_valid = pd.read_csv('data/CBT_CN_valid.csv',delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 17s, sys: 1.14 s, total: 2min 18s\n",
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sample_train['Document'] = sample_train['Document'].apply(literal_eval)\n",
    "sample_valid['Document'] = sample_valid['Document'].apply(literal_eval)\n",
    "sample_train['Query'] = sample_train['Query'].apply(literal_eval)\n",
    "sample_valid['Query'] = sample_valid['Query'].apply(literal_eval)\n",
    "sample_train['Candidates'] = sample_train['Candidates'].apply(literal_eval)\n",
    "sample_valid['Candidates'] = sample_valid['Candidates'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# transform list of words to their indices\n",
    "def categorize_text(text, word_to_id, mapped_words):\n",
    "    newtext = []\n",
    "    text_to_process = text\n",
    "    if type(text) is not list:\n",
    "        text_to_process = [text]\n",
    "    for word in text_to_process:\n",
    "        if mapped_words is None or word in mapped_words:\n",
    "            newtext.append(word_to_id[word])\n",
    "        else:\n",
    "            newtext.append(word_to_id['<NA>'])\n",
    "    return newtext\n",
    "\n",
    "def categorize_df(df, word_to_id=None):\n",
    "    cat_df = pd.DataFrame(dtype=str).reindex_like(df)\n",
    "    cat_df['Document'] = [[]] * len(df)\n",
    "    cat_df['Query'] = [[]] * len(df)\n",
    "    cat_df['Candidates'] = [[]] * len(df)\n",
    "    cat_df['Answer'] = ['<NA>'] * len(df)\n",
    "    words = []\n",
    "    mapped_words = None\n",
    "    id_to_word = None\n",
    "    if word_to_id is None:\n",
    "        print('Processing train data...')\n",
    "        words += list(itertools.chain.from_iterable(df['Document'].values))\n",
    "        words += list(itertools.chain.from_iterable(df['Query'].values))\n",
    "        words += list(itertools.chain.from_iterable(df['Candidates'].values))\n",
    "        words += list(df['Answer'].values)\n",
    "        print('\\t random word:', words[19374])\n",
    "        words += ['<NA>']\n",
    "        words = set(words)\n",
    "        print('\\t dictionary size(with NA):', len(words))\n",
    "        word_to_id = {t: i for i, t in enumerate(words)}\n",
    "        id_to_word = {i: t for i, t in enumerate(words)}\n",
    "    else:\n",
    "        print('Processing test data...')\n",
    "        mapped_words = set(word_to_id.keys())\n",
    "    \n",
    "    cat_df['Document'] = df['Document'].apply(lambda row: categorize_text(row, word_to_id, mapped_words))\n",
    "    cat_df['Query'] = df['Query'].apply(lambda row: categorize_text(row, word_to_id, mapped_words))\n",
    "    cat_df['Candidates'] = df['Candidates'].apply(lambda row: categorize_text(row, word_to_id, mapped_words))\n",
    "    cat_df['Answer'] = df['Answer'].apply(lambda row: categorize_text(row, word_to_id, mapped_words)[0])\n",
    "    \n",
    "    return cat_df, len(words), word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_categorization(id_to_word, df, cat_df, isTrain=True):\n",
    "    print('Checking categorization...')\n",
    "    ind_s = np.random.randint(0, high=len(df), size=100)\n",
    "    allClear = True\n",
    "    NA_freq = 0\n",
    "    tot_len = 0\n",
    "    for i in ind_s:\n",
    "        for col in df.columns[:-1]:\n",
    "            temp = cat_df[col].iloc[i]\n",
    "            assert type(temp) is list\n",
    "            tot_len += len(temp)\n",
    "            for j in range(len(temp)):\n",
    "                if id_to_word[temp[j]] != df[col].iloc[i][j]:\n",
    "                    if isTrain or id_to_word[temp[j]] != '<NA>':\n",
    "                        allClear = False\n",
    "                    else:\n",
    "                        NA_freq += 1\n",
    "        tot_len += 1\n",
    "        if id_to_word[cat_df['Answer'].iloc[i]] != df['Answer'].iloc[i]:\n",
    "            if isTrain or id_to_word[cat_df['Answer'].iloc[i]] != '<NA>':\n",
    "                allClear = False\n",
    "            else:\n",
    "                NA_freq += 1\n",
    "    NA_freq = 100.0 * float(NA_freq) / float(tot_len)\n",
    "    if allClear:\n",
    "        print('\\t Sector is clear')\n",
    "        if NA_freq > 0:\n",
    "            print('\\t Percentage of <NA> words:', NA_freq)\n",
    "    else:\n",
    "        print('\\t ... Not clear! Not clear!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train data...\n",
      "\t random word: too\n",
      "\t dictionary size(with NA): 42246\n",
      "Checking categorization...\n",
      "\t Sector is clear\n"
     ]
    }
   ],
   "source": [
    "cat_train, dictionary_size, word_to_id, id_to_word = categorize_df(sample_train)\n",
    "check_categorization(id_to_word, sample_train, cat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data...\n",
      "Checking categorization...\n",
      "\t Sector is clear\n",
      "\t Percentage of <NA> words: 0.8508800345535039\n"
     ]
    }
   ],
   "source": [
    "cat_valid, _, _, _ = categorize_df(sample_valid, word_to_id)\n",
    "check_categorization(id_to_word, sample_valid, cat_valid, isTrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Query</th>\n",
       "      <th>Candidates</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[21208, 18336, 36403, 37681, 41172, 15806, 278...</td>\n",
       "      <td>[16838, 5773, 27599, 38830, 5773, 19355, 38301...</td>\n",
       "      <td>[39207, 19720, 1070, 14617, 17864, 34787, 2715...</td>\n",
       "      <td>8119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[21208, 18336, 36403, 37681, 41172, 15806, 278...</td>\n",
       "      <td>[16838, 5773, 8119, 38830, 5773, 27599, 38301,...</td>\n",
       "      <td>[40271, 19720, 19355, 38509, 34787, 18542, 311...</td>\n",
       "      <td>19355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[21208, 18336, 36403, 37681, 41172, 15806, 278...</td>\n",
       "      <td>[16838, 5773, 8119, 38830, 5773, 19355, 38301,...</td>\n",
       "      <td>[39207, 40271, 10628, 24077, 12051, 823, 36403...</td>\n",
       "      <td>40271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[1237, 8340, 5773, 19355, 1817, 33083, 14575, ...</td>\n",
       "      <td>[18399, 32458, 2276, 37733, 8150, 29269, 37917...</td>\n",
       "      <td>[40271, 39465, 1070, 31738, 12051, 24437, 3980...</td>\n",
       "      <td>19355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[1237, 8340, 5773, 19355, 1817, 33083, 14575, ...</td>\n",
       "      <td>[18399, 32458, 2276, 37733, 8150, 29269, 37917...</td>\n",
       "      <td>[40271, 39465, 1070, 24077, 31738, 22089, 3113...</td>\n",
       "      <td>1070</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Document  \\\n",
       "0  [21208, 18336, 36403, 37681, 41172, 15806, 278...   \n",
       "1  [21208, 18336, 36403, 37681, 41172, 15806, 278...   \n",
       "2  [21208, 18336, 36403, 37681, 41172, 15806, 278...   \n",
       "3  [1237, 8340, 5773, 19355, 1817, 33083, 14575, ...   \n",
       "4  [1237, 8340, 5773, 19355, 1817, 33083, 14575, ...   \n",
       "\n",
       "                                               Query  \\\n",
       "0  [16838, 5773, 27599, 38830, 5773, 19355, 38301...   \n",
       "1  [16838, 5773, 8119, 38830, 5773, 27599, 38301,...   \n",
       "2  [16838, 5773, 8119, 38830, 5773, 19355, 38301,...   \n",
       "3  [18399, 32458, 2276, 37733, 8150, 29269, 37917...   \n",
       "4  [18399, 32458, 2276, 37733, 8150, 29269, 37917...   \n",
       "\n",
       "                                          Candidates  Answer  \n",
       "0  [39207, 19720, 1070, 14617, 17864, 34787, 2715...    8119  \n",
       "1  [40271, 19720, 19355, 38509, 34787, 18542, 311...   19355  \n",
       "2  [39207, 40271, 10628, 24077, 12051, 823, 36403...   40271  \n",
       "3  [40271, 39465, 1070, 31738, 12051, 24437, 3980...   19355  \n",
       "4  [40271, 39465, 1070, 24077, 31738, 22089, 3113...    1070  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120769, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Batching and utils\n",
    "(How to batch data from train/valid + some util functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample_batch(data, batch_size, word_to_id, D_max_len=1000, Q_max_len=100, offset=None):\n",
    "    if offset is None:\n",
    "        inds = random.sample(range(len(data)), batch_size)\n",
    "    else:\n",
    "        inds = range(offset, offset + batch_size)\n",
    "    D_len = np.array([min(len(arr), D_max_len) for arr in data.iloc[inds]['Document'].values])\n",
    "    Q_len = np.array([min(len(arr), Q_max_len) for arr in data.iloc[inds]['Query'].values])\n",
    "    \n",
    "    # D is document indices padded with '<NA>', same for Q\n",
    "    # we use DQ_mask to indicate what part of matching scores matrix to take\n",
    "    D, Q, DQ_mask, y = [], [], [], []\n",
    "    for arr in data.iloc[inds].values:\n",
    "        c_D_len, c_Q_len = len(arr[0]), len(arr[1])\n",
    "        if c_D_len < D_max_len:\n",
    "            # pad too short\n",
    "            D += [arr[0] + [word_to_id['<NA>']]*(D_max_len - len(arr[0]))]\n",
    "        else:\n",
    "            # crop too long\n",
    "            c_D_len = D_max_len\n",
    "            D += [arr[0][0:D_max_len]]\n",
    "        \n",
    "        if c_Q_len < Q_max_len:\n",
    "            Q += [arr[1] + [word_to_id['<NA>']]*(Q_max_len - len(arr[1]))]\n",
    "        else:\n",
    "            c_Q_len = Q_max_len\n",
    "            Q += [arr[1][0:Q_max_len]]\n",
    "        \n",
    "        DQ_mask += [list(np.pad(np.ones([c_D_len, c_Q_len]), (0, max(D_max_len-c_D_len, Q_max_len-c_Q_len)), \n",
    "                                'constant', constant_values=(0,0))[:D_max_len, :Q_max_len])]\n",
    "        # mark all places where y(answer word) is in Document\n",
    "        y += [list(np.array(np.array(D[-1]) == arr[3],dtype=int))]\n",
    "    D, Q = np.array(D), np.array(Q)\n",
    "    DQ_mask, y = np.array(DQ_mask,dtype=float), np.array(y,dtype=float)\n",
    "    return D, D_len, Q, Q_len, DQ_mask, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 108 ms, sys: 36 ms, total: 144 ms\n",
      "Wall time: 144 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# check time performance\n",
    "sample_dict_ex = sample_batch(cat_train, 32, word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 1000, 100) (32, 1000)\n",
      "(array([  0,   0,   0, ..., 278, 278, 278]), array([ 0,  1,  2, ..., 13, 14, 15]))\n",
      "279 16\n"
     ]
    }
   ],
   "source": [
    "DQ_mask_ex, y_ex = sample_dict_ex[4], sample_dict_ex[5]\n",
    "print(DQ_mask_ex.shape, y_ex.shape)\n",
    "print(np.where(DQ_mask_ex[0] == 1))\n",
    "print(sample_dict_ex[1][0], sample_dict_ex[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(M, axis, mask, EPS=1e-12, name=None):\n",
    "    with tf.name_scope(name, 'softmax', [M]):\n",
    "        max_axis = tf.reduce_max(M, axis, keep_dims=True)\n",
    "        M_exp = tf.exp(M - max_axis) * mask\n",
    "        norm = tf.reduce_sum(M_exp, axis, keep_dims=True)\n",
    "        return M_exp / (norm + EPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(model, data):\n",
    "    y_pred = model.predict(data,100)\n",
    "    accuracy = np.sum(y_pred[:,0] == data['Answer'])\n",
    "    print('accuracy:', accuracy/len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model\n",
    "We'll implement Attention-over-attention reader, as described in https://arxiv.org/abs/1607.04423"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Attention_over_Attention_Reader():\n",
    "    def __init__(self, dict_size, embedding_dim=100, hidden_state_dim=50, \n",
    "                 D_max=1000, Q_max=100, l2_w=0.0001, dropout_rate=0.1,\n",
    "                 adam_w=0.001, grad_clip=5):\n",
    "        self.dict_size = dict_size\n",
    "        self.emb_dim, self.rnn_dim = embedding_dim, hidden_state_dim\n",
    "        self.D_max, self.Q_max = D_max, Q_max\n",
    "        self.l2_w, self.adam_w = l2_w, adam_w\n",
    "        self.dropout, self.grad_clip = 1 - dropout_rate, grad_clip\n",
    "        self.comp_graph = tf.Graph()\n",
    "        self.init_graph()\n",
    "        \n",
    "        # initialize embedding matrix and rnn\n",
    "        self.sess = tf.Session(graph=self.comp_graph)\n",
    "        self.sess.run(self.init_all_op)\n",
    "    \n",
    "    def init_params(self):\n",
    "        self.D = tf.placeholder(tf.int32, [None, self.D_max], name='Document')\n",
    "        self.lenD = tf.placeholder(tf.int32, [None], name='Document_length')\n",
    "        self.Q = tf.placeholder(tf.int32, [None, self.Q_max], name='Query')\n",
    "        self.lenQ = tf.placeholder(tf.int32, [None], name='Query_length')\n",
    "        self.DQ_mask = tf.placeholder(tf.float32, \n",
    "                                      [None, self.D_max, self.Q_max], name='Document_Query_mask')\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.D_max], name='Answer_mask')\n",
    "        \n",
    "        self.embedding_mtx = tf.Variable(\n",
    "            tf.random_uniform([self.dict_size, self.emb_dim], \n",
    "                              -0.05, 0.05, dtype=tf.float32), name='Embedding_matrix')\n",
    "    \n",
    "    def process_text(self):\n",
    "        # Embed input texts\n",
    "        embedded_D = tf.nn.dropout(\n",
    "            tf.nn.embedding_lookup(self.embedding_mtx, self.D, name='Embedded_document'), \n",
    "            self.dropout)\n",
    "        embedded_Q = tf.nn.dropout(\n",
    "            tf.nn.embedding_lookup(self.embedding_mtx, self.Q, name='Embedded_query'), \n",
    "            self.dropout)\n",
    "        # Process Document using bi-GRU\n",
    "        with tf.variable_scope('Document_processor', initializer=tf.orthogonal_initializer()):\n",
    "            fwd_cell = tf.nn.rnn_cell.GRUCell(self.rnn_dim)\n",
    "            bwd_cell = tf.nn.rnn_cell.GRUCell(self.rnn_dim)\n",
    "            \n",
    "            h_out, _ = tf.nn.bidirectional_dynamic_rnn(fwd_cell, bwd_cell, embedded_D, \n",
    "                                         sequence_length=self.lenD, dtype=tf.float32)\n",
    "            self.h_Doc = tf.concat(h_out, 2)\n",
    "        # Process Query using bi-GRU\n",
    "        with tf.variable_scope('Query_processor', initializer=tf.orthogonal_initializer()):\n",
    "            fwd_cell = tf.nn.rnn_cell.GRUCell(self.rnn_dim)\n",
    "            bwd_cell = tf.nn.rnn_cell.GRUCell(self.rnn_dim)\n",
    "            \n",
    "            h_out, _ = tf.nn.bidirectional_dynamic_rnn(fwd_cell, bwd_cell, embedded_Q, \n",
    "                                         sequence_length=self.lenQ, dtype=tf.float32)\n",
    "            self.h_Query = tf.concat(h_out, 2)\n",
    "    \n",
    "    def compute_scores(self):\n",
    "        # get matching scores for Document and Query\n",
    "        M = tf.matmul(self.h_Doc, self.h_Query, transpose_b=True, name='Matching_scores')\n",
    "        # apply softmax Document-wise\n",
    "        self.alpha = softmax(M, 1, self.DQ_mask, name='Query_to_Document_attention')\n",
    "        # apply softmax Query-wise, then average to get importance of each word in Query\n",
    "        self.beta = tf.reduce_sum(softmax(M, 2, self.DQ_mask), \n",
    "                                  1, keep_dims=True, name='Document_to_Query_attention')\n",
    "        self.beta_imp = tf.div(self.beta, tf.maximum(tf.reduce_sum(self.DQ_mask,axis=1,keep_dims=True),1))\n",
    "        self.s = tf.matmul(self.alpha, self.beta_imp, transpose_b=True, name='Final_scores')\n",
    "    \n",
    "    def init_graph(self):\n",
    "        tf.reset_default_graph()\n",
    "        with self.comp_graph.as_default():\n",
    "            self.init_params()\n",
    "            self.process_text()\n",
    "            self.compute_scores()\n",
    "            # get probability that y is the answer word\n",
    "            with tf.variable_scope('Aggregating_results'):\n",
    "                self.p_y = tf.reduce_sum(\n",
    "                    tf.reduce_sum(self.s, axis=2) * self.y, \n",
    "                    axis=1)\n",
    "            # train to maximize negative log loss of the answer word\n",
    "            self.loss = -tf.reduce_mean(tf.log(tf.maximum(self.p_y, 1e-12)))\n",
    "            self.loss += self.l2_w * tf.nn.l2_loss(self.embedding_mtx)\n",
    "            \n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.adam_w)\n",
    "            # use gradient clipping to avoid exploding\n",
    "            gvs = optimizer.compute_gradients(self.loss)\n",
    "            capped_gvs = [(tf.clip_by_value(grad, -self.grad_clip, self.grad_clip), \n",
    "                           var) for grad, var in gvs]\n",
    "            self.train_op = optimizer.apply_gradients(capped_gvs)\n",
    "            \n",
    "            self.init_all_op = tf.global_variables_initializer()\n",
    "    \n",
    "    def fit(self, data, n_steps, batches_per_step, batch_size=32, valid_data=None):\n",
    "        self.train_losses = []\n",
    "        self.valid_losses = []\n",
    "        for step in tqdm(range(n_steps)):\n",
    "            temp,temp2 = 0,0\n",
    "            for i in range(batches_per_step):\n",
    "                sample_data = sample_batch(data, batch_size, word_to_id,\n",
    "                                                 D_max_len=self.D_max, Q_max_len=self.Q_max)\n",
    "                inputs = [self.D, self.lenD, self.Q, self.lenQ, self.DQ_mask, self.y] \n",
    "                _, iloss = self.sess.run([self.train_op, self.loss], \n",
    "                                         feed_dict={i:d for i,d in zip(inputs,sample_data)})\n",
    "                assert not np.isinf(iloss)\n",
    "                temp += iloss\n",
    "            if valid_data is not None:\n",
    "                sample_data = sample_batch(valid_data, 1000, word_to_id,\n",
    "                                                 D_max_len=self.D_max, Q_max_len=self.Q_max)\n",
    "                inputs = [self.D, self.lenD, self.Q, self.lenQ, self.DQ_mask, self.y]\n",
    "                temp2 = self.sess.run(self.loss, \n",
    "                                      feed_dict={i:d for i,d in zip(inputs,sample_data)})\n",
    "                \n",
    "            self.train_losses.append([temp/batches_per_step])\n",
    "            if step != 0 and self.valid_losses[-1] < temp2 * 0.98:\n",
    "                print('valid loss has reached minimum')\n",
    "                break\n",
    "            self.valid_losses.append([temp2])\n",
    "    \n",
    "    def predict(self, data, batch_size=100):\n",
    "        y_hat = [[0,0]]*len(data)\n",
    "        for step in tqdm(range(0, len(data), batch_size)):\n",
    "            sample_data = sample_batch(data, batch_size, word_to_id,\n",
    "                                      self.D_max, self.Q_max, offset=step)\n",
    "            inputs = [self.D, self.lenD, self.Q, self.lenQ, self.DQ_mask, self.y]\n",
    "            \n",
    "            scores = np.sum(self.sess.run(self.s,\n",
    "                                   feed_dict={i:d for i,d in zip(inputs,sample_data)}),2)\n",
    "            \n",
    "            for i,doc in enumerate(sample_data[0]):\n",
    "                ans = doc[0]\n",
    "                p_ans = 0\n",
    "                p_tot = 0\n",
    "                for word in np.unique(doc):\n",
    "                    p_word = np.sum(scores[i][np.where(doc == word)])\n",
    "                    p_tot += p_word\n",
    "                    if p_word > p_ans:\n",
    "                        ans = word\n",
    "                        p_ans = p_word\n",
    "                y_hat[step + i] = [ans, p_ans]\n",
    "        return np.array(y_hat)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Att_Reader = Attention_over_Attention_Reader(dictionary_size)\n",
    "tf.summary.FileWriter(\"logs\", Att_Reader.comp_graph).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial accuracy:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:56<00:00,  2.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print('initial accuracy:')\n",
    "compute_accuracy(Att_Reader, cat_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "valid_losses = []\n",
    "valid_accuracies = []\n",
    "\n",
    "# 32 * 200 * 50 = 320,000\n",
    "# 32 * 100 takes ~5min, \n",
    "total_n_steps = 200\n",
    "batches_per_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [23:55<00:00, 287.06s/it]\n",
      "100%|██████████| 5/5 [20:28<00:00, 245.65s/it]\n",
      " 40%|████      | 2/5 [07:29<11:14, 224.82s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss has reached minimum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Exception in thread Thread-15:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/usr/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      " 40%|████      | 2/5 [08:06<12:09, 243.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss has reached minimum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_n_steps = 20\n",
    "for steps in range(0, total_n_steps, 5):\n",
    "    Att_Reader.fit(cat_train, 5, batches_per_step, valid_data=cat_valid)\n",
    "    train_losses += Att_Reader.train_losses\n",
    "    valid_losses += Att_Reader.valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [10:02<15:04, 301.37s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss has reached minimum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Exception in thread Thread-16:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/usr/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      " 40%|████      | 2/5 [08:36<12:55, 258.48s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss has reached minimum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [22:45<00:00, 273.09s/it]\n",
      "100%|██████████| 5/5 [53:12<00:00, 638.53s/it]\n"
     ]
    }
   ],
   "source": [
    "total_n_steps = 20\n",
    "for steps in range(0, total_n_steps, 5):\n",
    "    Att_Reader.fit(cat_train, 5, batches_per_step, valid_data=cat_valid)\n",
    "    train_losses += Att_Reader.train_losses\n",
    "    valid_losses += Att_Reader.valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [07:15<29:03, 435.82s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss has reached minimum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Exception in thread Thread-19:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/usr/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|██████████| 5/5 [33:29<00:00, 401.92s/it]\n",
      "100%|██████████| 5/5 [37:48<00:00, 453.76s/it]\n",
      "100%|██████████| 5/5 [37:25<00:00, 449.06s/it]\n"
     ]
    }
   ],
   "source": [
    "total_n_steps = 20\n",
    "for steps in range(0, total_n_steps, 5):\n",
    "    Att_Reader.fit(cat_train, 5, batches_per_step, valid_data=cat_valid)\n",
    "    train_losses += Att_Reader.train_losses\n",
    "    valid_losses += Att_Reader.valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [37:09<00:00, 445.82s/it]\n",
      "100%|██████████| 5/5 [45:20<00:00, 544.05s/it]\n",
      "100%|██████████| 5/5 [52:50<00:00, 634.16s/it]\n",
      "100%|██████████| 5/5 [39:18<00:00, 471.66s/it]\n"
     ]
    }
   ],
   "source": [
    "total_n_steps = 20\n",
    "for steps in range(0, total_n_steps, 5):\n",
    "    Att_Reader.fit(cat_train, 5, batches_per_step, valid_data=cat_valid)\n",
    "    train_losses += Att_Reader.train_losses\n",
    "    valid_losses += Att_Reader.valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [33:09<08:17, 497.44s/it]\n",
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss has reached minimum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[A\n",
      "Exception in thread Thread-26:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/usr/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|██████████| 5/5 [46:59<00:00, 563.90s/it]\n",
      "100%|██████████| 5/5 [1:03:06<00:00, 757.38s/it]\n",
      " 60%|██████    | 3/5 [35:34<23:43, 711.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid loss has reached minimum\n"
     ]
    }
   ],
   "source": [
    "total_n_steps = 20\n",
    "for steps in range(0, total_n_steps, 5):\n",
    "    Att_Reader.fit(cat_train, 5, batches_per_step, valid_data=cat_valid)\n",
    "    train_losses += Att_Reader.train_losses\n",
    "    valid_losses += Att_Reader.valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy after 100 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n",
      "Exception in thread Thread-28:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.5/threading.py\", line 914, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/tqdm/_tqdm.py\", line 144, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/usr/lib/python3.5/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n",
      "100%|██████████| 20/20 [01:07<00:00,  3.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.295\n"
     ]
    }
   ],
   "source": [
    "print('accuracy after 100 steps')\n",
    "compute_accuracy(Att_Reader, cat_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('accuracy after 100 steps')\n",
    "compute_accuracy(Att_Reader, cat_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "plt.title('Negative log loss', size=25)\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(valid_losses, label='valid')\n",
    "plt.legend(loc='best',fontsize=20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
