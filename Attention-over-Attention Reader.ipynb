{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "import itertools\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_train = pd.read_csv('data/CBT_CN_train.csv',delimiter=';')\n",
    "sample_train = sample_train.sample(n=10000, replace=True)\n",
    "sample_valid = pd.read_csv('data/CBT_CN_valid.csv',delimiter=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_train['Document'] = sample_train['Document'].apply(literal_eval)\n",
    "sample_valid['Document'] = sample_valid['Document'].apply(literal_eval)\n",
    "sample_train['Query'] = sample_train['Query'].apply(literal_eval)\n",
    "sample_valid['Query'] = sample_valid['Query'].apply(literal_eval)\n",
    "sample_train['Candidates'] = sample_train['Candidates'].apply(literal_eval)\n",
    "sample_valid['Candidates'] = sample_valid['Candidates'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def categorize_text(text, word_to_id, mapped_words):\n",
    "    newtext = []\n",
    "    text_to_process = text\n",
    "    if type(text) is not list:\n",
    "        text_to_process = [text]\n",
    "    for word in text_to_process:\n",
    "        if mapped_words is None or word in mapped_words:\n",
    "            newtext.append(word_to_id[word])\n",
    "        else:\n",
    "            newtext.append(word_to_id['<NA>'])\n",
    "    return newtext\n",
    "\n",
    "def categorize_df(df, word_to_id=None):\n",
    "    cat_df = pd.DataFrame(dtype=str).reindex_like(df)\n",
    "    cat_df['Document'] = [[]] * len(df)\n",
    "    cat_df['Query'] = [[]] * len(df)\n",
    "    cat_df['Candidates'] = [[]] * len(df)\n",
    "    cat_df['Answer'] = ['<NA>'] * len(df)\n",
    "    words = []\n",
    "    mapped_words = None\n",
    "    id_to_word = None\n",
    "    if word_to_id is None:\n",
    "        print('Processing train data...')\n",
    "        words += list(itertools.chain.from_iterable(df['Document'].values))\n",
    "        words += list(itertools.chain.from_iterable(df['Query'].values))\n",
    "        words += list(itertools.chain.from_iterable(df['Candidates'].values))\n",
    "        words += list(df['Answer'].values)\n",
    "        print('\\t random word:', words[19374])\n",
    "        words += ['<NA>']\n",
    "        words = set(words)\n",
    "        print('\\t dictionary size(with NA):', len(words))\n",
    "        word_to_id = {t: i for i, t in enumerate(words)}\n",
    "        id_to_word = {i: t for i, t in enumerate(words)}\n",
    "    else:\n",
    "        print('Processing test data...')\n",
    "        mapped_words = set(word_to_id.keys())\n",
    "    \n",
    "    cat_df['Document'] = df['Document'].apply(lambda row: categorize_text(row, word_to_id, mapped_words))\n",
    "    cat_df['Query'] = df['Query'].apply(lambda row: categorize_text(row, word_to_id, mapped_words))\n",
    "    cat_df['Candidates'] = df['Candidates'].apply(lambda row: categorize_text(row, word_to_id, mapped_words))\n",
    "    cat_df['Answer'] = df['Answer'].apply(lambda row: categorize_text(row, word_to_id, mapped_words)[0])\n",
    "    \n",
    "    return cat_df, len(words), word_to_id, id_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_categorization(id_to_word, df, cat_df, isTrain=True):\n",
    "    print('Checking categorization...')\n",
    "    ind_s = np.random.randint(0, high=len(df), size=100)\n",
    "    allClear = True\n",
    "    NA_freq = 0\n",
    "    tot_len = 0\n",
    "    for i in ind_s:\n",
    "        for col in df.columns[:-1]:\n",
    "            temp = cat_df[col].iloc[i]\n",
    "            assert type(temp) is list\n",
    "            tot_len += len(temp)\n",
    "            for j in range(len(temp)):\n",
    "                if id_to_word[temp[j]] != df[col].iloc[i][j]:\n",
    "                    if isTrain or id_to_word[temp[j]] != '<NA>':\n",
    "                        allClear = False\n",
    "                    else:\n",
    "                        NA_freq += 1\n",
    "        tot_len += 1\n",
    "        if id_to_word[cat_df['Answer'].iloc[i]] != df['Answer'].iloc[i]:\n",
    "            if isTrain or id_to_word[cat_df['Answer'].iloc[i]] != '<NA>':\n",
    "                allClear = False\n",
    "            else:\n",
    "                NA_freq += 1\n",
    "    NA_freq = 100.0 * float(NA_freq) / float(tot_len)\n",
    "    if allClear:\n",
    "        print('\\t Sector is clear')\n",
    "        if NA_freq > 0:\n",
    "            print('\\t Percentage of <NA> words:', NA_freq)\n",
    "    else:\n",
    "        print('\\t ... Not clear! Not clear!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing train data...\n",
      "\t random word: of\n",
      "\t dictionary size(with NA): 33648\n",
      "Checking categorization...\n",
      "\t Sector is clear\n"
     ]
    }
   ],
   "source": [
    "cat_train, dictionary_size, word_to_id, id_to_word = categorize_df(sample_train)\n",
    "check_categorization(id_to_word, sample_train, cat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test data...\n",
      "Checking categorization...\n",
      "\t Sector is clear\n",
      "\t Percentage of <NA> words: 0.7929182748783353\n"
     ]
    }
   ],
   "source": [
    "cat_valid, _, _, _ = categorize_df(sample_valid, word_to_id)\n",
    "check_categorization(id_to_word, sample_valid, cat_valid, isTrain=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "inds_ex = range(0, 10000)\n",
    "temp = np.array([len(arr) for arr in cat_train.iloc[inds_ex]['Document'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9776"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(temp < 800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdb250dd9b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(temp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document</th>\n",
       "      <th>Query</th>\n",
       "      <th>Candidates</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80118</th>\n",
       "      <td>[9905, 12051, 24430, 23400, 584, 16078, 12051,...</td>\n",
       "      <td>[23610, 12051, 4752, 27129, 12315, 20829, 1872...</td>\n",
       "      <td>[28163, 30033, 4905, 31316, 2265, 1672, 584, 2...</td>\n",
       "      <td>4905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105129</th>\n",
       "      <td>[6357, 25317, 33112, 215, 19026, 18970, 2254, ...</td>\n",
       "      <td>[14257, 26964, 23610, 26964, 28320, 13449, 269...</td>\n",
       "      <td>[13449, 12424, 17375, 24830, 798, 2254, 2232, ...</td>\n",
       "      <td>5588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47932</th>\n",
       "      <td>[23896, 7007, 23400, 23489, 19928, 25933, 2943...</td>\n",
       "      <td>[2943, 21629, 8988, 7007, 11261, 27564, 26964,...</td>\n",
       "      <td>[23489, 8638, 7228, 7676, 32898, 32689, 22788,...</td>\n",
       "      <td>22788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39922</th>\n",
       "      <td>[9081, 23400, 31054, 26766, 23400, 16788, 1206...</td>\n",
       "      <td>[16485, 12069, 23400, 1368, 21337, 82, 21096, ...</td>\n",
       "      <td>[8326, 16788, 11493, 15389, 259, 7112, 12292, ...</td>\n",
       "      <td>259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13399</th>\n",
       "      <td>[12176, 10476, 16749, 26964, 11333, 2943, 2500...</td>\n",
       "      <td>[18084, 12069, 30227, 32877, 306, 26964, 6357,...</td>\n",
       "      <td>[30424, 13631, 26437, 21096, 20708, 26679, 278...</td>\n",
       "      <td>19982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Document  \\\n",
       "80118   [9905, 12051, 24430, 23400, 584, 16078, 12051,...   \n",
       "105129  [6357, 25317, 33112, 215, 19026, 18970, 2254, ...   \n",
       "47932   [23896, 7007, 23400, 23489, 19928, 25933, 2943...   \n",
       "39922   [9081, 23400, 31054, 26766, 23400, 16788, 1206...   \n",
       "13399   [12176, 10476, 16749, 26964, 11333, 2943, 2500...   \n",
       "\n",
       "                                                    Query  \\\n",
       "80118   [23610, 12051, 4752, 27129, 12315, 20829, 1872...   \n",
       "105129  [14257, 26964, 23610, 26964, 28320, 13449, 269...   \n",
       "47932   [2943, 21629, 8988, 7007, 11261, 27564, 26964,...   \n",
       "39922   [16485, 12069, 23400, 1368, 21337, 82, 21096, ...   \n",
       "13399   [18084, 12069, 30227, 32877, 306, 26964, 6357,...   \n",
       "\n",
       "                                               Candidates  Answer  \n",
       "80118   [28163, 30033, 4905, 31316, 2265, 1672, 584, 2...    4905  \n",
       "105129  [13449, 12424, 17375, 24830, 798, 2254, 2232, ...    5588  \n",
       "47932   [23489, 8638, 7228, 7676, 32898, 32689, 22788,...   22788  \n",
       "39922   [8326, 16788, 11493, 15389, 259, 7112, 12292, ...     259  \n",
       "13399   [30424, 13631, 26437, 21096, 20708, 26679, 278...   19982  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13449, 12424, 17375, 24830, 798, 2254, 2232, 5588, 16978, 20998]\n",
      "[23489, 8638, 7228, 7676, 32898, 32689, 22788, 1672, 12292, 9795]\n",
      "[8326, 16788, 11493, 15389, 259, 7112, 12292, 17413, 16883, 1039]\n"
     ]
    }
   ],
   "source": [
    "inds_x = [1, 2, 3]\n",
    "for arr in cat_train.iloc[inds_x].values:\n",
    "    print(arr[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(np.array([1,2,2]) == 2, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def sample_batch(data, batch_size, word_to_id, D_max_len=900, Q_max_len=100, offset=None):\n",
    "    if offset is None:\n",
    "        inds = random.sample(range(len(data)), batch_size)\n",
    "    else:\n",
    "        inds = range(offset, offset + batch_size)\n",
    "    D_lengths = np.array([min(len(arr), D_max_len) for arr in data.iloc[inds]['Document'].values])\n",
    "    Q_lengths = np.array([min(len(arr), Q_max_len) for arr in data.iloc[inds]['Query'].values])\n",
    "    D,Q,C,y = [],[],[],[]\n",
    "    for arr in data.iloc[inds].values:\n",
    "        if len(arr[0]) < D_max_len:\n",
    "            D += [arr[0] + [word_to_id['<NA>']]*(D_max_len - len(arr[0]))]\n",
    "        else:\n",
    "            D += [arr[0][0:D_max_len]]\n",
    "        \n",
    "        if len(arr[1]) < Q_max_len:\n",
    "            Q += [arr[1] + [word_to_id['<NA>']]*(Q_max_len - len(arr[1]))]\n",
    "        else:\n",
    "            Q += [arr[1][0:Q_max_len]]\n",
    "        C += [[list(np.array(D[-1]) == val) for val in arr[2]]]\n",
    "        #C += [arr[2]]\n",
    "        y += [list(np.array(np.array(arr[2]) == arr[3],dtype=int))]\n",
    "    D, Q = np.array(D), np.array(Q)\n",
    "    C,y = np.array(C,dtype=int),np.array(y)\n",
    "    return D, Q, C, y, D_lengths, Q_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "D_ex, Q_ex, C_ex, y_ex, D_l_ex, Q_l_ex = sample_batch(cat_train, 3, word_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 27 238 253 293]\n",
      "[ 99 280]\n",
      "[135]\n",
      "[233 248]\n",
      "[ 77 203]\n",
      "[213]\n",
      "[332]\n",
      "[277]\n",
      "[291]\n",
      "[197 210]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(np.where(C_ex[0][i] == 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 10, 900)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C_ex.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 3)\n",
      "[[ 0.5         0.2         0.30000001]\n",
      " [ 0.40000001  0.40000001  0.2       ]]\n",
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "A = tf.placeholder(tf.float32, [None, None])\n",
    "B = tf.placeholder(tf.float32, [None, 3, None])\n",
    "\n",
    "pos = tf.reduce_sum(tf.multiply(tf.expand_dims(A,axis=1), B), axis=2)\n",
    "#pos = tf.gather(params=A, indices=B, axis=1)\n",
    "print(pos.shape)\n",
    "\n",
    "B_ex = np.array([[[1,0,0,1],[0,1,0,0],[0,0,1,0]],[[0,1,0,0],[0,0,1,0],[1,0,0,1]]])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    pos_ex = sess.run(pos, {A: np.array([[0.1, 0.2, 0.3, 0.4], [0.2, 0.4, 0.4, 0.0]]),\n",
    "                            B: B_ex})\n",
    "    print(pos_ex)\n",
    "    print(pos_ex.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 128, 50) (?, 16, 50)\n",
      "(?, 128, 50) (?, 16, 50)\n",
      "(?, 128, 16)\n",
      "(?, 128, 16)\n",
      "(?, 16)\n",
      "(?, 10)\n"
     ]
    }
   ],
   "source": [
    "emb_dim = 50\n",
    "rnn_dim = 50\n",
    "\n",
    "tf.reset_default_graph()\n",
    "g = tf.Graph()\n",
    "\n",
    "with g.as_default():\n",
    "    D = tf.placeholder(tf.int32, [None, 128], name='Document')\n",
    "    len_D = tf.placeholder(tf.int32, [None], name='Document_lengths')\n",
    "    Q = tf.placeholder(tf.int32, [None, 16], name='Query')\n",
    "    len_Q = tf.placeholder(tf.int32, [None], name='Query_lengths')\n",
    "    C = tf.placeholder(tf.float32, [None, 10, 128], name='Candidates')\n",
    "    y = tf.placeholder(tf.int32, [None, 10], name='Answers')\n",
    "\n",
    "    W_e = tf.Variable(tf.random_uniform([dictionary_size, emb_dim], -0.05, 0.05, \n",
    "                                    dtype=tf.float32), name='Embedding_matrix')\n",
    "    e_D = tf.nn.embedding_lookup(W_e, D, name='Embedded_document')\n",
    "    e_Q = tf.nn.embedding_lookup(W_e, Q, name='Embedded_query')\n",
    "    print(e_D.shape, e_Q.shape)\n",
    "    \n",
    "    with tf.variable_scope('Document_processor'):\n",
    "        basic_cell = tf.nn.rnn_cell.GRUCell(rnn_dim, activation=tf.nn.relu)\n",
    "        h_Doc, _ = tf.nn.dynamic_rnn(basic_cell, e_D, sequence_length=len_D, dtype=tf.float32)\n",
    "    with tf.variable_scope('Query_processor'):\n",
    "        basic_cell = tf.nn.rnn_cell.GRUCell(rnn_dim, activation=tf.nn.relu)\n",
    "        h_Query, _ = tf.nn.dynamic_rnn(basic_cell, e_Q, sequence_length=len_Q, dtype=tf.float32)\n",
    "    \n",
    "    print(h_Doc.shape, h_Query.shape)\n",
    "    M = tf.matmul(h_Doc, h_Query, transpose_b=True, name='Matching_scores')\n",
    "    print(M.shape)\n",
    "    \n",
    "    with tf.variable_scope('Query_to_Doc_Attention'):\n",
    "        alpha = tf.nn.softmax(M, dim=1, name='Query_to_Document_attention')\n",
    "    with tf.variable_scope('Doc_to_Query_Attention'):\n",
    "        beta = tf.reduce_sum(tf.nn.softmax(M, dim=2, name='Document_to_Query_attention'), \n",
    "                             axis=1, name='Avg_doc_to_Query_att')\n",
    "    s = tf.reduce_sum(tf.multiply(alpha, beta), axis=2, name='Final_scores')\n",
    "    \n",
    "    with tf.variable_scope('Aggregating_results'):\n",
    "        logits = tf.reduce_sum(tf.multiply(tf.expand_dims(s,axis=1), C), axis=2)\n",
    "    \n",
    "    loss = tf.losses.softmax_cross_entropy(logits=logits, onehot_labels=y)\n",
    "    \n",
    "    train_op = tf.train.AdamOptimizer().minimize(loss, )\n",
    "    \n",
    "    print(alpha.shape)\n",
    "    print(beta.shape)\n",
    "    print(logits.shape)\n",
    "\n",
    "tf.summary.FileWriter(\"logs\", g).close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.trainable_variables()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
